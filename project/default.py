# -*- coding: utf-8 -*-
"""Copy of Default.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P9A15uwB2nPCR3avKooxndln8gXDYuFS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from xgboost.sklearn import XGBClassifier
from xgboost.sklearn import XGBModel
import xgboost as xgb
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_auc_score

from google.colab import files
uploaded = files.upload()

card = pd.read_csv("defaults.csv")

card.columns

card.tail()

card.shape

card.describe()

card.isna().sum()

card.info()

card = card.rename(columns = {'default payment next month': 'default_next_month'}, inplace = False)

card.columns

card['default_next_month'].value_counts()

sns.countplot(card['default_next_month'])

#where 1 is left and 0 is stayed
cus_no_default = card[card.default_next_month== 0].shape[0]
cus_default = card[card.default_next_month ==1].shape[0]

#printing percentage of customers that stayed
print(cus_no_default/(cus_no_default+ cus_default) * 100, '% of customers stayed with the company.' )

#printing percentage of customers that left
print(cus_default/(cus_no_default+ cus_default) * 100, '% of customers left with the company.' )

plt.figure(figsize=(20,10))
matrix = np.triu(card.corr())
plt.title('Correlational Heatmap')
sns.heatmap(card.corr(), annot=True, mask=matrix, fmt='.1g')

#where 1 represents male and 2, women.
sns.countplot(x='SEX', hue='default_next_month', data=card)

"""We have more women that might default in next month's payment."""

plt.figure(figsize=(15,8))
sns.histplot(x='LIMIT_BAL', hue='default_next_month', data=card)

"""We observe at lower limit balances we have more customers that will most like default in next months payment."""

sns.set_style('ticks')
fig, ax = plt.subplots()
# the size of A4 paper
fig.set_size_inches(11.7, 8.27)
sns.countplot(x='EDUCATION', hue='default_next_month', data=card)
plt.xlabel("Education")
plt.ylabel("Default Next Month's Payment")
plt.title("Relationship between defaulting and education")

"""We observe that university students take the most loans and have the highest probability of defaulting in next month's payment."""

numerical_ft = ['LIMIT_BAL', 'AGE']
fig, ax = plt.subplots(1,2, figsize=(28, 8))
card[card.default_next_month == 0][numerical_ft].hist(bins=20, color='blue', alpha=0.5, ax = ax)
card[card.default_next_month == 1][numerical_ft].hist(bins=20, color='orange', alpha=0.5, ax = ax)

"""From the graphs above it was observed that most of the defaulters had lower limit balances and customers had low credit limits. It was also observed that customers with the age range of 21-34 would mostly like default in next months payments."""

card['AGE'].value_counts()

"""## PREPROCESSING"""

card = card.drop('ID', axis = 1)

card.head()

card.rename(columns={'default_next_month':'default'}, inplace=True)



from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve
from sklearn.preprocessing import RobustScaler

target_name = 'default'
X = card.drop('default', axis=1)
robust_scaler = RobustScaler()
X = robust_scaler.fit_transform(X)
y = card[target_name]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=123, stratify=y)

X_train

def CMatrix(CM, labels=['pay','default']):
    df = pd.DataFrame(data=CM, index=labels, columns=labels)
    df.index.name='TRUE'
    df.columns.name='PREDICTION'
    df.loc['Total'] = df.sum()
    df['Total'] = df.sum(axis=1)
    return df

# Data frame for evaluation metrics
metrics = pd.DataFrame(index=['accuracy', 'precision', 'recall'], 
                      columns=['NULL','LogisticReg', 'ClassTree', 'NaiveBayes', 'Xgboost', 'RF'])

y_pred_test = np.repeat(y_train.value_counts().idxmax(), y_test.size)
metrics.loc['accuracy','NULL'] = accuracy_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['precision','NULL'] = precision_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['recall','NULL'] = recall_score(y_pred=y_pred_test, y_true=y_test)

CM = confusion_matrix(y_pred=y_pred_test, y_true=y_test)
CMatrix(CM)

"""# Logistic

"""

# 1. Import the estimator object (model)
from sklearn.linear_model import LogisticRegression

# 2. Create an instance of the estimator
logistic_regression = LogisticRegression(n_jobs=-1, random_state=15)

# 3. Use the trainning data to train the estimator
logistic_regression.fit(X_train, y_train)

# 4. Evaluate the model
y_pred_test = logistic_regression.predict(X_test)
metrics.loc['accuracy','LogisticReg'] = accuracy_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['precision','LogisticReg'] = precision_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['recall','LogisticReg'] = recall_score(y_pred=y_pred_test, y_true=y_test)
#Confusion matrix
CM = confusion_matrix(y_pred=y_pred_test, y_true=y_test)
CMatrix(CM)

"""# classification trees"""

# 1. Import the estimator object (model)
from sklearn.tree import DecisionTreeClassifier

# 2. Create an instance of the estimator
class_tree = DecisionTreeClassifier(min_samples_split=30, min_samples_leaf=10, random_state=10)

# 3. Use the trainning data to train the estimator
class_tree.fit(X_train, y_train)

# 4. Evaluate the model
y_pred_test = class_tree.predict(X_test)
metrics.loc['accuracy','ClassTree'] = accuracy_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['precision','ClassTree'] = precision_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['recall','ClassTree'] = recall_score(y_pred=y_pred_test, y_true=y_test)
#Confusion matrix
CM = confusion_matrix(y_pred=y_pred_test, y_true=y_test)
CMatrix(CM)

"""# Naive Bayes"""

# 1. Import the estimator object (model)
from sklearn.naive_bayes import GaussianNB

# 2. Create an instance of the estimator
NBC = GaussianNB()

# 3. Use the trainning data to train the estimator
NBC.fit(X_train, y_train)

# 4. Evaluate the model
y_pred_test = NBC.predict(X_test)
metrics.loc['accuracy','NaiveBayes'] = accuracy_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['precision','NaiveBayes'] = precision_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['recall','NaiveBayes'] = recall_score(y_pred=y_pred_test, y_true=y_test)

#Confusion matrix
CM = confusion_matrix(y_pred=y_pred_test, y_true=y_test)
CMatrix(CM)

"""# RF"""

# 1. Import the estimator object (model)
from sklearn.ensemble import RandomForestClassifier

# 2. Create an instance of the estimator
RF = RandomForestClassifier()

# 3. Use the trainning data to train the estimator
RF.fit(X_train, y_train)

# 4. Evaluate the model
y_pred_test = RF.predict(X_test)
metrics.loc['accuracy','RF'] = accuracy_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['precision','RF'] = precision_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['recall','RF'] = recall_score(y_pred=y_pred_test, y_true=y_test)

#Confusion matrix
CM = confusion_matrix(y_pred=y_pred_test, y_true=y_test)
CMatrix(CM)

"""# XGBOOST"""

# 1. Import the estimator object (model)
from xgboost.sklearn import XGBClassifier

# 2. Create an instance of the estimator
Xgboost = XGBClassifier()

# 3. Use the trainning data to train the estimator
Xgboost.fit(X_train, y_train)

# 4. Evaluate the model
y_pred_test = Xgboost.predict(X_test)
metrics.loc['accuracy','Xgboost'] = accuracy_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['precision','Xgboost'] = precision_score(y_pred=y_pred_test, y_true=y_test)
metrics.loc['recall','Xgboost'] = recall_score(y_pred=y_pred_test, y_true=y_test)

#Confusion matrix
CM = confusion_matrix(y_pred=y_pred_test, y_true=y_test)
CMatrix(CM)

100*metrics

fig, ax = plt.subplots(figsize=(8,5))
metrics.plot(kind='barh', ax=ax)
ax.grid();

precision_nb, recall_nb, thresholds_nb = precision_recall_curve(y_true=y_test, 
                                                                probas_pred=NBC.predict_proba(X_test)[:,1])
precision_Xgboost, recall_Xgboost, thresholds_Xgboost = precision_recall_curve(y_true=y_test, 
                                                                probas_pred=Xgboost.predict_proba(X_test)[:,1])

fig, ax = plt.subplots(figsize=(8,5))
ax.plot(precision_nb, recall_nb, label='NaiveBayes')
ax.plot(precision_Xgboost, recall_Xgboost, label='Xgboost')
ax.set_xlabel('Precision')
ax.set_ylabel('Recall')
ax.set_title('Precision-Recall Curve')
ax.hlines(y=0.5, xmin=0, xmax=1, color='red')
ax.legend()
ax.grid();

fig, ax = plt.subplots(figsize=(8,5))
ax.plot(thresholds_Xgboost, precision_Xgboost[1:], label='Precision')
ax.plot(thresholds_Xgboost, recall_Xgboost[1:], label='Recall')
ax.set_xlabel('Classification Threshold')
ax.set_ylabel('Precision, Recall')
ax.set_title('Xgboost Classifier: Precision-Recall')
ax.hlines(y=0.6, xmin=0, xmax=1, color='red')
ax.legend()
ax.grid();

"""By looking at the precision-recall curve we observe that the precision and recall lines meet at 0.28. Giving us a better recall and precision value as we as a better F1 score."""

y_pred_proba =Xgboost.predict_proba(X_test)[:,1]
y_pred_test = (y_pred_proba >= 0.28).astype('int')
#Confusion matrix
CM = confusion_matrix(y_pred=y_pred_test, y_true=y_test)
print("Recall: ", 100*recall_score(y_pred=y_pred_test, y_true=y_test))
print("Precision: ", 100*precision_score(y_pred=y_pred_test, y_true=y_test))
CMatrix(CM)

"""By adjusting the classification threshold we observe a better recall and precision value."""

Xgboost = xgb.XGBClassifier()

Xgboost.fit(X,y)

print(f'Accuracy - : {Xgboost.score(X,y):.3f}')

pred_result = Xgboost.predict(X_test)
print(pred_result)

import pickle

filename = 'new_model'
pickle.dump(Xgboost, open(filename, 'wb'))

from google.colab import files
files.download(filename)

loaded_model = pickle.load(open(filename, 'rb'))
loaded_model.predict(X_test)

"""# Predicting Default"""

def make_ind_prediction(new_data):
    data = new_data.values.reshape(1, -1)
    data = robust_scaler.transform(data)
    prob = Xgboost.predict_proba(data)[0][1]
    if prob >= 0.2:
        return 'Will default'
    else:
        return 'Will pay'

from collections import OrderedDict
new_customer = OrderedDict([('LIMIT_BAL', 4000),('SEX', 2 ),('EDUCATION', 2),
                            ('MARRIAGE', 1 ),('AGE', 50 ),('PAY_0', 0 ),
                            ('PAY_2', 0 ),('PAY_3', 0 ), ('PAY_4', -1 ),('PAY_5', -1 ),
                            ('PAY_6', 0 ),('BILL_AMT1', 3921 ),('BILL_AMT2', 2000 ), ('BILL_AMT3', 2000 ),
                            ('BILL_AMT4', 1000 ),('BILL_AMT5', 2000 ),('BILL_AMT6', 1500 ), ('PAY_AMT1', 0 ),
                            ('PAY_AMT2', 1000 ),('PAY_AMT3', 4000 ),('PAY_AMT4', 2000 ),('PAY_AMT5', 3000 ),
                            ('PAY_AMT6', 100)])

new_customer = pd.Series(new_customer)
make_ind_prediction(new_customer)